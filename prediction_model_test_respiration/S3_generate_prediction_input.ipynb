{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "import time \n",
    "# %% move to current file path\n",
    "os.chdir(sys.path[0])\n",
    "\n",
    "# %%\n",
    "with open(os.path.join(\"OPs_used\", \"bloodConc.json\"), \"r\") as f:\n",
    "    bloodConc = json.load(f)\n",
    "    bloodConc = bloodConc['bloodConc']\n",
    "with open(os.path.join(\"OPs_used\", \"wavelength.json\"), 'r') as f:\n",
    "    wavelength = json.load(f)\n",
    "    wavelength = wavelength['wavelength']\n",
    "with open(os.path.join(\"OPs_used\", \"SO2.json\"), 'r') as f:\n",
    "    SO2 = json.load(f)\n",
    "    train_SO2 = SO2['train_SO2']\n",
    "    test_SO2 = SO2['test_SO2']\n",
    "with open(os.path.join('OPs_used', \"muscle_SO2.json\"), 'r') as f:\n",
    "    muscle_SO2 = json.load(f)\n",
    "    muscle_SO2 = muscle_SO2['SO2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_precition_input(wl2_idx, wl_idx, all_dataset_large, all_dataset_small, based_ijv_SO2):\n",
    "    SDS1_dataset_large = all_dataset_large[all_dataset_large['wavelength']==wavelength[wl_idx]]\n",
    "    SDS1_dataset_small = all_dataset_small[all_dataset_small['wavelength']==wavelength[wl_idx]]\n",
    "    SDS2_dataset_large = all_dataset_large[all_dataset_large['wavelength']==wavelength[wl2_idx]]\n",
    "    SDS2_dataset_small = all_dataset_small[all_dataset_small['wavelength']==wavelength[wl2_idx]]\n",
    "    output_large = []\n",
    "    output_small = []\n",
    "    for blc in bloodConc:\n",
    "        for used_ijv_SO2 in test_SO2:\n",
    "            R_T1_large_SDS1 = SDS1_dataset_large[(SDS1_dataset_large['bloodConc']==blc) & (SDS1_dataset_large['used_SO2']==based_ijv_SO2)]['SDS_1'].to_numpy()\n",
    "            R_T1_large_SDS2 = SDS2_dataset_large[(SDS2_dataset_large['bloodConc']==blc) & (SDS2_dataset_large['used_SO2']==based_ijv_SO2)]['SDS_11'].to_numpy()\n",
    "            \n",
    "            R_T1_small_SDS1 = SDS1_dataset_small[(SDS1_dataset_small['bloodConc']==blc) & (SDS1_dataset_small['used_SO2']==based_ijv_SO2)]['SDS_1'].to_numpy()\n",
    "            R_T1_small_SDS2 = SDS2_dataset_small[(SDS2_dataset_small['bloodConc']==blc) & (SDS2_dataset_small['used_SO2']==based_ijv_SO2)]['SDS_11'].to_numpy()\n",
    "            \n",
    "            R_T2_large_SDS1 = SDS1_dataset_large[(SDS1_dataset_large['bloodConc']==blc) & (SDS1_dataset_large['used_SO2']==used_ijv_SO2)]['SDS_1'].to_numpy()\n",
    "            R_T2_large_SDS2 = SDS2_dataset_large[(SDS2_dataset_large['bloodConc']==blc) & (SDS2_dataset_large['used_SO2']==used_ijv_SO2)]['SDS_11'].to_numpy()\n",
    "            \n",
    "            R_T2_small_SDS1 = SDS1_dataset_small[(SDS1_dataset_small['bloodConc']==blc) & (SDS1_dataset_small['used_SO2']==used_ijv_SO2)]['SDS_1'].to_numpy()\n",
    "            R_T2_small_SDS2= SDS2_dataset_small[(SDS2_dataset_small['bloodConc']==blc) & (SDS2_dataset_small['used_SO2']==used_ijv_SO2)]['SDS_11'].to_numpy()\n",
    "            \n",
    "            output_large += list((R_T2_large_SDS1/R_T2_large_SDS2) - (R_T1_large_SDS1/R_T1_large_SDS2))\n",
    "            output_small += list((R_T2_small_SDS1/R_T2_small_SDS2)- (R_T1_small_SDS1/R_T1_small_SDS2))\n",
    "    \n",
    "    return wl2_idx, wl_idx, output_large, output_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now processing mus_type : low, ijv_depth : standard, ijv_size : -10%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [06:57<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish spectrum = 427.5501048564911 s\n",
      "finish spectrum 2 = 427.5530982017517 s\n",
      "finish save all = 447.78075098991394 s\n",
      "finish save high = 455.4735760688782 s\n",
      "finish save mid = 462.22964358329773 s\n",
      "finish save low = 467.2696189880371 s\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# def save_prediction_input(prediction_input : pd, start : int, end : int, condition : int):\n",
    "#     data = []\n",
    "#     count = 0\n",
    "#     for i in range(condition):\n",
    "#         for r in range(start, end):\n",
    "#             if count == 0:\n",
    "#                 data = prediction_input[prediction_input['id']==f\"{i}_{r}\"]\n",
    "#             else:\n",
    "#                 data = pd.concat((data, prediction_input[prediction_input['id']==f\"{i}_{r}\"]))\n",
    "#             count += 1\n",
    "#     return data\n",
    "# %%\n",
    "# ijv_depth = ['+1mm', '+0.5mm', '-0.5mm', '-1mm', 'standard']\n",
    "ijv_depth = ['standard']\n",
    "# ijv_size = ['-50%', '-30%', '-20%', '-10%', 'standard']\n",
    "ijv_size = ['-10%']\n",
    "# mus_types = ['low', 'medium', 'high']\n",
    "mus_types = ['low']\n",
    "subject = 'ctchen'\n",
    "start_time = time.time()\n",
    "# %%\n",
    "for using_depth in ijv_depth:\n",
    "    for using_size in ijv_size:\n",
    "        for mus_type in mus_types:       \n",
    "            print(f'Now processing mus_type : {mus_type}, ijv_depth : {using_depth}, ijv_size : {using_size}')\n",
    "            os.makedirs(os.path.join('dataset', subject, f'ijv_depth_{using_depth}', f'ijv_size_{using_size}', f'{mus_type}_scatter_prediction_input', 'low_absorption'), exist_ok=True)\n",
    "            os.makedirs(os.path.join('dataset', subject, f'ijv_depth_{using_depth}', f'ijv_size_{using_size}', f'{mus_type}_scatter_prediction_input', 'medium_absorption'), exist_ok=True)\n",
    "            os.makedirs(os.path.join('dataset', subject, f'ijv_depth_{using_depth}', f'ijv_size_{using_size}', f'{mus_type}_scatter_prediction_input', 'high_absorption'), exist_ok=True)\n",
    "            os.makedirs(os.path.join('dataset', subject, f'ijv_depth_{using_depth}', f'ijv_size_{using_size}', f'{mus_type}_scatter_prediction_input', 'all_absorption'), exist_ok=True)\n",
    "\n",
    "            all_dataset_large = pd.read_csv(os.path.join('dataset', subject, f'{subject}_dataset_large_ijv_depth_{using_depth}_size_{using_size}', f'{mus_type}', 'large_all.csv'))\n",
    "            all_dataset_small = pd.read_csv(os.path.join('dataset', subject, f'{subject}_dataset_small_ijv_depth_{using_depth}_size_{using_size}', f'{mus_type}', 'small_all.csv'))\n",
    "            \n",
    "            # %%\n",
    "            based_ijv_SO2 = 0.7\n",
    "            prediction_input = {}\n",
    "            for j in range(len(wavelength)):\n",
    "                for i in range(len(wavelength)):\n",
    "                    prediction_input[f'large_{wavelength[i]}nm_based_on_{wavelength[j]}'] = []\n",
    "            for j in range(len(wavelength)):\n",
    "                for i in range(len(wavelength)):\n",
    "                    prediction_input[f'small_{wavelength[i]}nm_based_on_{wavelength[j]}'] = []\n",
    "            # for i in range(len(wavelength)):\n",
    "            #     prediction_input[f'T2_large_{wavelength[i]}nm'] = []\n",
    "            # for i in range(len(wavelength)):\n",
    "            #     prediction_input[f'T2_small_{wavelength[i]}nm'] = []\n",
    "            prediction_input['blc'] = []\n",
    "            prediction_input['ijv_SO2_change'] = []\n",
    "            prediction_input['id'] = []\n",
    "            prediction_input['mua_rank'] = []\n",
    "\n",
    "            products = []\n",
    "            for wl2_idx in range(len(wavelength)):\n",
    "                for wl_idx in range(len(wavelength)):\n",
    "                    products.append((wl2_idx,wl_idx))\n",
    "\n",
    "            res = Parallel(n_jobs=-5)(delayed(gen_precition_input)(wl2_idx, wl_idx, all_dataset_large, all_dataset_small, based_ijv_SO2) for wl2_idx, wl_idx in tqdm(products))\n",
    "            for wl2_idx, wl_idx, output_large, output_small in res:\n",
    "                prediction_input[f'large_{wavelength[wl_idx]}nm_based_on_{wavelength[wl2_idx]}'] += output_large\n",
    "                prediction_input[f'small_{wavelength[wl_idx]}nm_based_on_{wavelength[wl2_idx]}'] += output_small\n",
    "            \n",
    "            print(f'finish spectrum = {time.time()-start_time} s')\n",
    "            start_time = time.time()\n",
    "            \n",
    "            count = 0\n",
    "            for blc in bloodConc:\n",
    "                for used_ijv_SO2 in test_SO2:\n",
    "                    prediction_input['blc'] += [blc]*20\n",
    "                    prediction_input['ijv_SO2_change'] += [used_ijv_SO2-based_ijv_SO2]*20\n",
    "                    prediction_input['id'] += [count]*20\n",
    "                    prediction_input['mua_rank'] += [i for i in range(20)]\n",
    "                    count += 1\n",
    "            print(f'finish spectrum 2 = {time.time()-start_time} s')\n",
    "            start_time = time.time()\n",
    "\n",
    "            prediction_input = pd.DataFrame(prediction_input)\n",
    "            prediction_input.to_csv(os.path.join('dataset', subject, f'ijv_depth_{using_depth}', f'ijv_size_{using_size}', f'{mus_type}_scatter_prediction_input', 'all_absorption', 'prediction_input.csv'), index=False)\n",
    "            all_prediction_input = prediction_input.to_numpy()\n",
    "            np.save(os.path.join('dataset', subject, f'ijv_depth_{using_depth}', f'ijv_size_{using_size}', f'{mus_type}_scatter_prediction_input', 'all_absorption', 'prediction_input.npy'), all_prediction_input)\n",
    "            condition = count\n",
    "            print(f'finish save all = {time.time()-start_time} s')\n",
    "            start_time = time.time()\n",
    "            # %%\n",
    "            # data = save_prediction_input(prediction_input, start=0, end=7, condition=condition)\n",
    "            data = prediction_input[prediction_input['mua_rank']<= 7]\n",
    "            data.to_csv(os.path.join('dataset', subject, f'ijv_depth_{using_depth}', f'ijv_size_{using_size}', f'{mus_type}_scatter_prediction_input', 'high_absorption', 'prediction_input.csv'), index=False)\n",
    "            data = data.to_numpy()\n",
    "            np.save(os.path.join('dataset', subject, f'ijv_depth_{using_depth}', f'ijv_size_{using_size}', f'{mus_type}_scatter_prediction_input', 'high_absorption', 'prediction_input.npy'), data)\n",
    "            print(f'finish save high = {time.time()-start_time} s')\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # data = save_prediction_input(prediction_input, start=7, end=14, condition=condition)\n",
    "            data = prediction_input[(prediction_input['mua_rank']>7) & (prediction_input['mua_rank']<=14)]\n",
    "            data.to_csv(os.path.join('dataset', subject, f'ijv_depth_{using_depth}', f'ijv_size_{using_size}', f'{mus_type}_scatter_prediction_input', 'medium_absorption', 'prediction_input.csv'), index=False)\n",
    "            data = data.to_numpy()\n",
    "            np.save(os.path.join('dataset', subject, f'ijv_depth_{using_depth}', f'ijv_size_{using_size}', f'{mus_type}_scatter_prediction_input', 'medium_absorption', 'prediction_input.npy'), data)\n",
    "            print(f'finish save mid = {time.time()-start_time} s')\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # data = save_prediction_input(prediction_input, start=14, end=20, condition=condition)\n",
    "            data = prediction_input[(prediction_input['mua_rank']>14) & (prediction_input['mua_rank']<=20)]\n",
    "            data.to_csv(os.path.join('dataset', subject, f'ijv_depth_{using_depth}', f'ijv_size_{using_size}', f'{mus_type}_scatter_prediction_input', 'low_absorption', 'prediction_input.csv'), index=False)\n",
    "            data = data.to_numpy()\n",
    "            np.save(os.path.join('dataset', subject, f'ijv_depth_{using_depth}', f'ijv_size_{using_size}', f'{mus_type}_scatter_prediction_input', 'low_absorption', 'prediction_input.npy'), data)\n",
    "            print(f'finish save low = {time.time()-start_time} s')\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
