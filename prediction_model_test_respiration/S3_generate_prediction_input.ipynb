{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "from joblib import Parallel, delayed\n",
    "# %% move to current file path\n",
    "os.chdir(sys.path[0])\n",
    "\n",
    "# %%\n",
    "with open(os.path.join(\"OPs_used\", \"bloodConc.json\"), \"r\") as f:\n",
    "    bloodConc = json.load(f)\n",
    "    bloodConc = bloodConc['bloodConc']\n",
    "with open(os.path.join(\"OPs_used\", \"wavelength.json\"), 'r') as f:\n",
    "    wavelength = json.load(f)\n",
    "    wavelength = wavelength['wavelength']\n",
    "with open(os.path.join(\"OPs_used\", \"SO2.json\"), 'r') as f:\n",
    "    SO2 = json.load(f)\n",
    "    train_SO2 = SO2['train_SO2']\n",
    "    test_SO2 = SO2['test_SO2']\n",
    "with open(os.path.join('OPs_used', \"muscle_SO2.json\"), 'r') as f:\n",
    "    muscle_SO2 = json.load(f)\n",
    "    muscle_SO2 = muscle_SO2['SO2']\n",
    "# %%\n",
    "# def save_prediction_input(prediction_input : pd, start : int, end : int, condition : int):\n",
    "#     data = []\n",
    "#     count = 0\n",
    "#     for i in range(condition):\n",
    "#         for r in range(start, end):\n",
    "#             if count == 0:\n",
    "#                 data = prediction_input[prediction_input['id']==f\"{i}_{r}\"]\n",
    "#             else:\n",
    "#                 data = pd.concat((data, prediction_input[prediction_input['id']==f\"{i}_{r}\"]))\n",
    "#             count += 1\n",
    "#     return data\n",
    "# %%\n",
    "# ijv_depth = ['+1mm', '+0.5mm', '-0.5mm', '-1mm', 'standard']\n",
    "ijv_depth = ['standard']\n",
    "ijv_size = ['-50%', '-30%', '-20%', '-10%', 'standard']\n",
    "# mus_types = ['low', 'medium', 'high']\n",
    "mus_types = ['low']\n",
    "subject = 'ctchen'\n",
    "# %%\n",
    "for using_depth in ijv_depth:\n",
    "    for using_size in ijv_size:\n",
    "        for mus_type in mus_types:       \n",
    "            print(f'Now processing mus_type : {mus_type}, ijv_depth : {using_depth}, ijv_size : {using_size}')\n",
    "            os.makedirs(os.path.join('dataset', subject, f'ijv_depth_{using_depth}', f'ijv_size_{using_size}', f'{mus_type}_scatter_prediction_input', 'low_absorption'), exist_ok=True)\n",
    "            os.makedirs(os.path.join('dataset', subject, f'ijv_depth_{using_depth}', f'ijv_size_{using_size}', f'{mus_type}_scatter_prediction_input', 'medium_absorption'), exist_ok=True)\n",
    "            os.makedirs(os.path.join('dataset', subject, f'ijv_depth_{using_depth}', f'ijv_size_{using_size}', f'{mus_type}_scatter_prediction_input', 'high_absorption'), exist_ok=True)\n",
    "            os.makedirs(os.path.join('dataset', subject, f'ijv_depth_{using_depth}', f'ijv_size_{using_size}', f'{mus_type}_scatter_prediction_input', 'all_absorption'), exist_ok=True)\n",
    "\n",
    "            # %%\n",
    "            based_ijv_SO2 = 0.7\n",
    "            prediction_input = {}\n",
    "            for j in range(len(wavelength)):\n",
    "                for i in range(len(wavelength)):\n",
    "                    prediction_input[f'large_{wavelength[i]}nm_based_on_{wavelength[j]}'] = []\n",
    "            for j in range(len(wavelength)):\n",
    "                for i in range(len(wavelength)):\n",
    "                    prediction_input[f'small_{wavelength[i]}nm_based_on_{wavelength[j]}'] = []\n",
    "            # for i in range(len(wavelength)):\n",
    "            #     prediction_input[f'T2_large_{wavelength[i]}nm'] = []\n",
    "            # for i in range(len(wavelength)):\n",
    "            #     prediction_input[f'T2_small_{wavelength[i]}nm'] = []\n",
    "            prediction_input['blc'] = []\n",
    "            prediction_input['ijv_SO2_change'] = []\n",
    "            prediction_input['id'] = []\n",
    "            prediction_input['mua_rank'] = []\n",
    "\n",
    "            count = 0\n",
    "            for wl2_idx in range(len(wavelength)):\n",
    "                for wl_idx in range(len(wavelength)):\n",
    "                    SDS1_dataset_large = pd.read_csv(os.path.join('dataset', subject, f'{subject}_dataset_large_ijv_depth_{using_depth}_size_{using_size}', f'{mus_type}', f'{wavelength[wl_idx]}nm_mus_{wl_idx+1}.csv'))\n",
    "                    SDS1_dataset_small = pd.read_csv(os.path.join('dataset', subject, f'{subject}_dataset_small_ijv_depth_{using_depth}_size_{using_size}', f'{mus_type}', f'{wavelength[wl_idx]}nm_mus_{wl_idx+1}.csv'))\n",
    "                    SDS2_dataset_large = pd.read_csv(os.path.join('dataset', subject, f'{subject}_dataset_large_ijv_depth_{using_depth}_size_{using_size}', f'{mus_type}', f'{wavelength[wl2_idx]}nm_mus_{wl2_idx+1}.csv'))\n",
    "                    SDS2_dataset_small = pd.read_csv(os.path.join('dataset', subject, f'{subject}_dataset_small_ijv_depth_{using_depth}_size_{using_size}', f'{mus_type}', f'{wavelength[wl2_idx]}nm_mus_{wl2_idx+1}.csv'))\n",
    "                    for blc in bloodConc:\n",
    "                        for used_ijv_SO2 in test_SO2:\n",
    "                            R_T1_large_SDS1 = SDS1_dataset_large[(SDS1_dataset_large['bloodConc']==blc) & (SDS1_dataset_large['used_SO2']==based_ijv_SO2)]['SDS_1']\n",
    "                            R_T1_large_SDS2 = SDS2_dataset_large[(SDS2_dataset_large['bloodConc']==blc) & (SDS2_dataset_large['used_SO2']==based_ijv_SO2)]['SDS_11']\n",
    "                            \n",
    "                            R_T1_small_SDS1 = SDS1_dataset_small[(SDS1_dataset_small['bloodConc']==blc) & (SDS1_dataset_small['used_SO2']==based_ijv_SO2)]['SDS_1']\n",
    "                            R_T1_small_SDS2 = SDS2_dataset_small[(SDS2_dataset_small['bloodConc']==blc) & (SDS2_dataset_small['used_SO2']==based_ijv_SO2)]['SDS_11']\n",
    "                            \n",
    "                            R_T2_large_SDS1 = SDS1_dataset_large[(SDS1_dataset_large['bloodConc']==blc) & (SDS1_dataset_large['used_SO2']==used_ijv_SO2)]['SDS_1']\n",
    "                            R_T2_large_SDS2 = SDS2_dataset_large[(SDS2_dataset_large['bloodConc']==blc) & (SDS2_dataset_large['used_SO2']==used_ijv_SO2)]['SDS_11']\n",
    "                            \n",
    "                            R_T2_small_SDS1 = SDS1_dataset_small[(SDS1_dataset_small['bloodConc']==blc) & (SDS1_dataset_small['used_SO2']==used_ijv_SO2)]['SDS_1']\n",
    "                            R_T2_small_SDS2= SDS2_dataset_small[(SDS2_dataset_small['bloodConc']==blc) & (SDS2_dataset_small['used_SO2']==used_ijv_SO2)]['SDS_11']\n",
    "                            \n",
    "                            prediction_input[f'large_{wavelength[wl_idx]}nm_based_on_{wavelength[wl2_idx]}'] += list((R_T2_large_SDS1/R_T2_large_SDS2).to_numpy() - (R_T1_large_SDS1/R_T1_large_SDS2).to_numpy())\n",
    "                            prediction_input[f'small_{wavelength[wl_idx]}nm_based_on_{wavelength[wl2_idx]}'] += list((R_T2_small_SDS1/R_T2_small_SDS2).to_numpy() - (R_T1_small_SDS1/R_T1_small_SDS2).to_numpy())\n",
    "                            # prediction_input[f'T2_large_{wavelength[wl_idx]}nm'] += list(R_T2_large_SDS1/R_T2_large_SDS2)\n",
    "                            # prediction_input[f'T2_small_{wavelength[wl_idx]}nm'] += list(R_T2_small_SDS1/R_T2_small_SDS2)\n",
    "                            \n",
    "                            # print(f'blc : {blc}, used_ijv_SO2 : {used_ijv_SO2}, used_muscle_SO2 : {used_muscle_SO2}, {R_T2.shape}')\n",
    "            for blc in bloodConc:\n",
    "                for used_ijv_SO2 in test_SO2:\n",
    "                    prediction_input['blc'] += [blc]*20\n",
    "                    prediction_input['ijv_SO2_change'] += [used_ijv_SO2-based_ijv_SO2]*20\n",
    "                    prediction_input['id'] += [count]*20\n",
    "                    prediction_input['mua_rank'] += [i for i in range(20)]\n",
    "                    count += 1\n",
    "\n",
    "\n",
    "            prediction_input = pd.DataFrame(prediction_input)\n",
    "            prediction_input.to_csv(os.path.join('dataset', subject, f'ijv_depth_{using_depth}', f'ijv_size_{using_size}', f'{mus_type}_scatter_prediction_input', 'all_absorption', 'prediction_input.csv'), index=False)\n",
    "            all_prediction_input = prediction_input.to_numpy()\n",
    "            np.save(os.path.join('dataset', subject, f'ijv_depth_{using_depth}', f'ijv_size_{using_size}', f'{mus_type}_scatter_prediction_input', 'all_absorption', 'prediction_input.npy'), all_prediction_input)\n",
    "            condition = count\n",
    "\n",
    "            # %%\n",
    "            # data = save_prediction_input(prediction_input, start=0, end=7, condition=condition)\n",
    "            data = prediction_input[prediction_input['mua_rank']<= 7]\n",
    "            data.to_csv(os.path.join('dataset', subject, f'ijv_depth_{using_depth}', f'ijv_size_{using_size}', f'{mus_type}_scatter_prediction_input', 'high_absorption', 'prediction_input.csv'), index=False)\n",
    "            data = data.to_numpy()\n",
    "            np.save(os.path.join('dataset', subject, f'ijv_depth_{using_depth}', f'ijv_size_{using_size}', f'{mus_type}_scatter_prediction_input', 'high_absorption', 'prediction_input.npy'), data)\n",
    "\n",
    "            # data = save_prediction_input(prediction_input, start=7, end=14, condition=condition)\n",
    "            data = prediction_input[(prediction_input['mua_rank']>7) & (prediction_input['mua_rank']<=14)]\n",
    "            data.to_csv(os.path.join('dataset', subject, f'ijv_depth_{using_depth}', f'ijv_size_{using_size}', f'{mus_type}_scatter_prediction_input', 'medium_absorption', 'prediction_input.csv'), index=False)\n",
    "            data = data.to_numpy()\n",
    "            np.save(os.path.join('dataset', subject, f'ijv_depth_{using_depth}', f'ijv_size_{using_size}', f'{mus_type}_scatter_prediction_input', 'medium_absorption', 'prediction_input.npy'), data)\n",
    "\n",
    "            # data = save_prediction_input(prediction_input, start=14, end=20, condition=condition)\n",
    "            data = prediction_input[(prediction_input['mua_rank']>14) & (prediction_input['mua_rank']<=20)]\n",
    "            data.to_csv(os.path.join('dataset', subject, f'ijv_depth_{using_depth}', f'ijv_size_{using_size}', f'{mus_type}_scatter_prediction_input', 'low_absorption', 'prediction_input.csv'), index=False)\n",
    "            data = data.to_numpy()\n",
    "            np.save(os.path.join('dataset', subject, f'ijv_depth_{using_depth}', f'ijv_size_{using_size}', f'{mus_type}_scatter_prediction_input', 'low_absorption', 'prediction_input.npy'), data)\n",
    "\n",
    "# products = []\n",
    "# for mus_type in mus_types:\n",
    "#     for muscle_type in muscle_types:\n",
    "#         products.append((mus_type,muscle_type))\n",
    "\n",
    "# Parallel(n_jobs=-5)(delayed(gen_precition_input)(mus_type, muscle_type) for mus_type, muscle_type in products)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
